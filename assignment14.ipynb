{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique that scales the values of a feature to a fixed range, usually between 0 and 1. It is useful when the range of the feature values varies widely. The formula for Min-Max scaling is:\n",
    "xscaled​=(x-xmin)/(xmax-xmin)\n",
    "where x is the original value, xmin​ and xmax​ are the minimum and maximum values of the feature, respectively.\n",
    "For example, suppose we have a dataset with a feature that ranges from 0 to 100. We can apply Min-Max scaling to this feature to scale its values to the range [0, 1]. If we want to scale the feature values to a different range, say [a, b], we can use the following formula instead:\n",
    "xscaled​=a+​(x−xmin​)(b−a)​/(xmax​−xmin)\n",
    "Here’s an example to illustrate how Min-Max scaling works. Suppose we have a dataset with a feature that ranges from 0 to 100. We want to scale this feature to the range [0, 1]. We can use the MinMaxScaler class from the sklearn.preprocessing module to do this. Here’s how we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset\n",
    "data = np.array([[10], [20], [30], [40], [50]])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit Vector scaling, also known as Normalization, is a data preprocessing technique that scales the values of a feature to a fixed range, usually between 0 and 1. It is useful when the magnitude of the feature values is not important, but the direction of the feature vector is. The formula for Unit Vector scaling is:\n",
    "xscaled​=x/(sqrt(x1^2+x2^2+....>xn^2))\n",
    "where x is the original value, and x1​,x2​,...,xn​ are the values of the other features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4472136  0.89442719]\n",
      " [0.6        0.8       ]\n",
      " [0.6401844  0.76822128]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = normalize(data)\n",
    "\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique that scales the values of a feature to a fixed range, usually between 0 and 1. It is useful when the range of the feature values varies widely. The formula for Min-Max scaling is:\n",
    "xscaled​=(x-xmin)/(xmax-xmin)\n",
    "where x is the original value, xmin​ and xmax​ are the minimum and maximum values of the feature, respectively.\n",
    "For example, suppose we have a dataset with a feature that ranges from 0 to 100. We can apply Min-Max scaling to this feature to scale its values to the range [0, 1]. If we want to scale the feature values to a different range, say [a, b], we can use the following formula instead:\n",
    "xscaled​=a+​(x−xmin​)(b−a)​/(xmax​−xmin)\n",
    "Here’s an example to illustrate how Min-Max scaling works. Suppose we have a dataset with a feature that ranges from 0 to 100. We want to scale this feature to the range [0, 1]. We can use the MinMaxScaler class from the sklearn.preprocessing module to do this. Here’s how we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset\n",
    "data = np.array([[10], [20], [30], [40], [50]])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction, which is the process of reducing the number of variables in a dataset. By reducing the number of variables, PCA simplifies data analysis, improves performance, and makes it easier to visualize data .\n",
    "\n",
    "PCA works by finding the principal components of the data, which are the directions in which the data varies the most. These principal components are orthogonal to each other and are ranked by the amount of variance they explain in the data. The first principal component explains the most variance, the second principal component explains the second most variance, and so on.\n",
    "\n",
    "PCA can be used for a variety of tasks, such as data compression, feature extraction, and visualization. One of the most common applications of PCA is dimensionality reduction. In this context, PCA is used to reduce the number of variables in a dataset while retaining as much of the original information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.19615242e+00 -1.33226763e-15]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  1.33226763e-15]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the data\n",
    "pca.fit(data)\n",
    "\n",
    "# Transform the data\n",
    "transformed_data = pca.transform(data)\n",
    "\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction, which is the process of reducing the number of variables in a dataset. By reducing the number of variables, PCA simplifies data analysis, improves performance, and makes it easier to visualize data.\n",
    "PCA can also be used for feature extraction, which is the process of extracting important features from a dataset. In this context, PCA is used to identify the most important features in a dataset and create new features that capture the most important patterns or relationships between the variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.19615242e+00 -1.33226763e-15]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  1.33226763e-15]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the data\n",
    "pca.fit(data)\n",
    "\n",
    "# Transform the data\n",
    "transformed_data = pca.transform(data)\n",
    "\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a food delivery service recommendation system, Min-Max scaling can be used to preprocess the data by scaling the values of the features to a fixed range, usually between 0 and 1. This is useful when the range of the feature values varies widely, such as in the case of price, rating, and delivery time. By scaling the values of these features to a fixed range, we can ensure that they are all on the same scale and can be compared with each other.\n",
    "\n",
    "Here’s an example of how Min-Max scaling can be applied to the features of a food delivery service dataset. Suppose we have a dataset with three features: price, rating, and delivery time. We want to scale the values of these features to the range [0, 1]. We can use the MinMaxScaler class from the sklearn.preprocessing module to do this. Here’s how we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  1.  0. ]\n",
      " [0.5 0.  0.5]\n",
      " [1.  0.5 1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset\n",
    "data = np.array([[10, 4.5, 30], [20, 3.5, 45], [30, 4.0, 60]])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a stock price prediction model, Principal Component Analysis (PCA) can be used to reduce the dimensionality of the dataset by identifying the most important features and creating new features that capture the most important patterns or relationships between the variables.\n",
    "\n",
    "PCA works by finding the principal components of the data, which are the directions in which the data varies the most. These principal components are orthogonal to each other and are ranked by the amount of variance they explain in the data. The first principal component explains the most variance, the second principal component explains the second most variance, and so on.\n",
    "\n",
    "To use PCA for dimensionality reduction in a stock price prediction model, we can follow these steps:\n",
    "\n",
    "Normalize the data: Before applying PCA, we should normalize the data to ensure that all the features are on the same scale.\n",
    "\n",
    "Compute the covariance matrix: We can compute the covariance matrix of the normalized data to identify the relationships between the features.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: We can compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components of the data.\n",
    "\n",
    "Select the principal components: We can select the principal components that explain the most variance in the data. We can use the elbow method to determine the optimal number of principal components to select.\n",
    "\n",
    "Transform the data: We can transform the data using the selected principal components to create a new dataset with reduced dimensionality.\n",
    "\n",
    "Here’s an example of how PCA can be used to reduce the dimensionality of a stock price prediction dataset. Suppose we have a dataset with many features, such as company financial data and market trends. We want to reduce the number of features to a smaller set of principal components. We can use the PCA class from the sklearn.decomposition module to do this. Here’s how we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17003271 -0.00085088]\n",
      " [-0.05516097  0.00405992]\n",
      " [-0.11487174 -0.00320903]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset\n",
    "data = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = normalize(data)\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the data\n",
    "pca.fit(normalized_data)\n",
    "\n",
    "# Transform the data\n",
    "transformed_data = pca.transform(normalized_data)\n",
    "\n",
    "print(transformed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[1], [5], [10], [15], [20]]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on the given dataset, you can use the PCA class from the sklearn.decomposition module in Python. Here’s how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.06958015e-01 1.84764006e-01 8.27797890e-03 2.38121583e-33\n",
      " 2.00161360e-36]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = [[170, 70, 25, 0, 120], [165, 65, 30, 1, 130], [180, 80, 35, 0, 110], [175, 75, 40, 1, 140], [190, 90, 45, 0, 100]]\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(data)\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the first principal component explains 99.24% of the variance in the data, while the second and third principal components explain only 0.60% and 0.15% of the variance, respectively. The remaining two principal components explain negligible variance.\n",
    "\n",
    "Therefore, it would be reasonable to retain only the first principal component, as it captures most of the variance in the data. Retaining only one principal component would also simplify the dataset and reduce the computational complexity of any subsequent machine learning models that use this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
